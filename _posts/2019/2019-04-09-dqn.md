---
layout: post    
title: 【论文阅读】DQN
categories: RL RL_Paper
description: 深度强化学习的鼻祖算法DQN
keywords: Machine Learning, Reinforcement Learning, DQN
---

DQN的论文地址：[Playing Atari with Deep Reinforcement Learning](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)

DQN属于是Q-learning算法，是一种无模型、异步异步、时间差分的控制算法，使用深度模型来拟合值函数。

DQN使用原始图片作为输入，就是原始图片的像素值，将每个动作的Q值作为输出。而对于输入来说，并不是每一帧都会作为输入，而是连续四帧的序列作为输入，因为从一帧难以捕捉太多信息。也就是说这个MDP中的状态表示是一个四帧的像素向量。

DQN使用了**经验回放**(experience replay)。对于在线的强化学习方法来说，就是代理边采取动作边观察来进行在线更新，这样会有一些问题，比如生成的样本会跟你的动作有关，或者说观察到的状态会受到你的动作执行的影响。那么经验回放是什么意思呢？通俗来说就是使用一个数据池来存储前面使用过的数据，后面还会用到这些数据。经验回放的优点：

- 每个数据样本可以被使用多次，提高采样效率
- 由于连续样本之间的强相关性，从连续的样本中学习不高效，而随机这些样本可以打破这种相关性，可以减少更新过程的不稳定性
- 如果是同步策略学习，下一个数据样本会受到当前参数的影响，可以使得陷入局部最小。比如有两个动作空间的游戏中，向左移动那么下一个样本状态受限于左手区域，反之向右移动下一个样本状态受限于右手区域。而经验回放可以有效解决这一问题。

同时在使用过程中，数据池的大小 $N$ 也是一个超参数，有相关[论文](https://pdfs.semanticscholar.org/049c/6719ac3470e03316d75b3d605b79eccd24e5.pdf)研究这个问题。

DQN算法的损失函数为

$$L_i(\theta_i) = E_{s,a \sim \rho(\cdot)}[(y_i - Q(s,a;\theta_i))^2]$$

梯度为

$$\triangledown_{\theta_i}L_i(\theta_i) = E_{s,a \sim \rho(\cdot);s' \sim \varepsilon}\left[\left(r + \gamma \underset{a'}{max}Q(s',a';\theta^{i - 1}) - Q(s,a;\theta_i)\right)\triangledown_{\theta_i}Q(s,a;\theta_i)\right]$$

其中 $\rho(\cdot)$ 表示状态s和动作a的概率分布， $\varepsilon$ 表示随机的MDP环境。

DQN算法的步骤为

![](https://github.com/feedliu/feedliu.github.io/blob/master/images/blog/DQN-algorithm.png?raw=true)

可以看到DQN算法在更新时并不是使用即时生成的样本来更新，而是从经验回放的数据池中随机抽取样本进行更新。

论文中实验的一些相关细节：

- 输入层为84\*84\*4的向量，第一个隐藏层为 16 个 8 \* 8 步长为 4 的卷积核以及 relu 激活函数，第二个隐藏层为 32 个 4 \* 4 步长为2的卷积核以及 relu 激活函数，最后一个隐藏层为 256 个单元的全连接层以及relu激活函数，输出层的单元数为动作空间的大小，并且为线性层
- 不同游戏使用相同的网络结构、学习算法、超参配置
- 在训练阶段，会调整奖励机制，正奖励设为1，负奖励设为-1，无奖励为0，因为不同的游戏奖励大小无法比较，这会影响到误差梯度的大小，从而不能使用相同的学习率
- 都是用RMSProp优化算法
- 批量大小为32
- $\epsilon-greedy$ 中的 $\epsilon$ 在前100万帧样本从1变到0.1，之后固定为0.1
- 训练样本量为1000万帧，经验回放大小为100万
- 每个$k = 4$帧进行动作选择，中间跳过的帧保持执行上一个选择的动作，因为动作选择耗时太长，如果每帧都选择动作时间上不允许

论文在7个Atari游戏中做了实验，其中六个超越了当前的所有方法，三个超越了人类水平。
